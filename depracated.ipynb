{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_CITATION = \"\"\"\\\n",
    "@inproceedings{LinWZE2018:NL2Bash, \n",
    "  author = {Xi Victoria Lin and Chenglong Wang and Luke Zettlemoyer and Michael D. Ernst}, \n",
    "  title = {NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System}, \n",
    "  booktitle = {Proceedings of the Eleventh International Conference on Language Resources\n",
    "               and Evaluation {LREC} 2018, Miyazaki (Japan), 7-12 May, 2018.},\n",
    "  year = {2018} \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "_DESCRIPTION = \"\"\"\\\n",
    "The dataset is constructed from\n",
    "https://github.com/TellinaTool/nl2bash\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (T5ForConditionalGeneration, T5Tokenizer, \n",
    "                          DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, \n",
    "                          Seq2SeqTrainer)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load Data and Inspect\n",
    "Load your JSON data and inspect its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "data = load_data('data/nl2bash-data.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Preprocess Data\n",
    "Convert your data into a format suitable for training. This might involve tokenization or other forms of preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    formatted_data = [f\"{value['invocation']} </s> {value['cmd']} </s>\" for key, value in data.items()]\n",
    "    return train_test_split(formatted_data, test_size=0.2)\n",
    "\n",
    "train_data, val_data = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Initialize Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the T5 base model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoding = tokenizer(\"Display current running kernel's compile-time config file.\",\"cat /boot/config-`uname -r`\")\n",
    "sample_encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11677, 750, 1180, 20563, 31, 7, 2890, 699, 18, 715, 3, 20303, 1042, 5, 1, 1712, 3, 87, 18475, 87, 20303, 18, 2, 76, 4350, 3, 18, 52, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "print(sample_encoding[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(sample_encoding[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Display current running kernel ' s comp ile - time  config file . </s> cat  / boot / config - <unk> u name  - r <unk> </s>\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [\n",
    "    tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    for g in sample_encoding[\"input_ids\"]\n",
    "]\n",
    "\" \".join(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 32128])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_test = tokenizer(\n",
    "    \"Display current running kernel's compile-time config file.\",\n",
    "    \"cat /boot/config-`uname -r`\", \n",
    "    max_length=512, \n",
    "    padding=\"max_length\", \n",
    "    return_tensors=\"pt\"\n",
    ")   \n",
    "\n",
    "output_test = model(\n",
    "    input_ids=input_test[\"input_ids\"],\n",
    "    attention_mask=input_test[\"attention_mask\"],\n",
    "    labels=input_test[\"input_ids\"]\n",
    ")\n",
    "\n",
    "output_test.logits.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.7304, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_test.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioQAModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        return output.loss, output.logits\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, _ = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, _ = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, _ = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BioQAModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # or another metric that you want to monitor\n",
    "    filename='checkpoint-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=3,  # number of best models to save\n",
    "    mode='min',  # 'min' for minimizing the monitored metric, 'max' for maximizing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/lucasoliveira/miniconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=3,\n",
    "    callbacks=[checkpoint_callback],  # Pass as a list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BioQAModel.__init__() takes 1 positional argument but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lucasoliveira/Documents/tgs-model/fine_tune_model.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lucasoliveira/Documents/tgs-model/fine_tune_model.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data_module \u001b[39m=\u001b[39m BioQAModel(train_data, val_data, tokenizer)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucasoliveira/Documents/tgs-model/fine_tune_model.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39mfit(model1, data_module)\n",
      "\u001b[0;31mTypeError\u001b[0m: BioQAModel.__init__() takes 1 positional argument but 4 were given"
     ]
    }
   ],
   "source": [
    "data_module = BioQAModel(train_data, val_data, tokenizer)\n",
    "trainer.fit(model1, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128, max_input_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.max_input_length = max_input_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        source_text, target_text = item.split(\" </s> \")\n",
    "        \n",
    "        source_tokenized = self.tokenizer(\n",
    "            source_text, \n",
    "            max_length=self.max_input_length, \n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "        target_tokenized = self.tokenizer(\n",
    "            target_text, \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": source_tokenized[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": source_tokenized[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": target_tokenized[\"input_ids\"].squeeze()\n",
    "        }\n",
    "\n",
    "# Custom collate function for dynamic padding\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    \n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "# dataset = CustomTextDataset(data, tokenizer)\n",
    "# dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomTextDataset(train_data, tokenizer)\n",
    "val_dataset = CustomTextDataset(val_data, tokenizer)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=32)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Print an example from the training dataset\n",
    "print(train_dataset[100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Initialize Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',  # NEW: Logging directory\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "\n",
    "print(train_dataset[0])\n",
    "print(val_dataset[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, tokenizer, sample_input_text):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device).eval()\n",
    "    input_tokenized = tokenizer.encode_plus(\n",
    "        sample_input_text, \n",
    "        max_length=512, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**input_tokenized, max_length=50, min_length=5, temperature=1.0)\n",
    "\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "sample_input_text = \"Display current running kernel's compile-time config file.\"\n",
    "print(test_model(model, tokenizer, sample_input_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"model\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"model\")\n",
    "tokenizer.save_pretrained(\"model\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
